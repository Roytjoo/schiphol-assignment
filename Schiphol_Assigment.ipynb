{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Schiphol Assigment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNINcKPVDWNLIAmRkJ8aULD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roytjoo/schiphol-assignment/blob/master/Schiphol_Assigment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSWQXOktKhMe"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "To create a demoable code I have chosen to use Google Codelab to show the code and a way to easily interact with Pyspark. For productionized code I definitly choose for Github and python packages and modules with in combination with Docker and Pycharm as IDE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0GOix5bI3IW",
        "outputId": "7d7a4f84-480b-458d-9c45-a1da944bd40a"
      },
      "source": [
        "# download and installation of PySpark, and installation of the packages findspark and itables\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q -N https://apache.newfountain.nl/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark itables pyspark_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g49XnZoX2X2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "268445ab-dd79-49c7-b347-1489e9dfb156"
      },
      "source": [
        "# load extension for datatables\n",
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The google.colab.data_table extension is already loaded. To reload it, use:\n",
            "  %reload_ext google.colab.data_table\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q08ziuafJdk7"
      },
      "source": [
        "# import os + setting variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSlJTrQcL6un"
      },
      "source": [
        "# import pyspark libraries and initialize Spark\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkFiles\n",
        "from pyspark.sql.dataframe import DataFrame \n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import col, window, udf, current_timestamp, row_number, lit\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
        "import time\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import urllib.request\n",
        "import math\n",
        "from itables import show\n",
        "from google.colab import data_table\n",
        "import pytest\n",
        "from pyspark_test import assert_pyspark_df_equal\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbzPxPYYIeJV"
      },
      "source": [
        "# set logging\n",
        "log = logging.getLogger(__name__)\n",
        "log.setLevel(\"INFO\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khFEvmLHN4hY"
      },
      "source": [
        "# Task 1\n",
        "\n",
        "*Create a batch Spark job that read in the routes dataset. It should create an overview of the top 10 airports used as source airport. Write the output to a filesystem.*\n",
        "\n",
        "I have split up the different parts into small function for maintainability reusability. I used docstrings to describe the functions and inserted logging for execution progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwvojrXkInHG"
      },
      "source": [
        "# create a schema for the file \n",
        "route_schema = StructType([ \\\n",
        "    StructField(\"Airline\", StringType(),True), \\\n",
        "    StructField(\"Airline ID\", StringType(),True), \\\n",
        "    StructField(\"Source airport\", StringType(),True), \\\n",
        "    StructField(\"Source airport ID\", StringType(), True), \\\n",
        "    StructField(\"Destination airport\", StringType(), True), \\\n",
        "    StructField(\"Destination airport ID\", StringType(), True), \\\n",
        "    StructField(\"Codeshare\", StringType(), True), \\\n",
        "    StructField(\"Stops\", IntegerType(), True), \\\n",
        "    StructField(\"Equipment\", StringType(), True)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqWmoPcIAiOW"
      },
      "source": [
        "def extract_data(file_name: str, header: bool=False, schema: StructType=route_schema):\n",
        "  \"\"\"\n",
        "  extracts data from a file by reading it into a spark datafarme\n",
        "  @param file_name: name of the file \n",
        "  @param header: boolean if a header is used\n",
        "  @param schema: the schema that should be used\n",
        "  \"\"\"\n",
        "  log.info(\"extract data from file\")\n",
        "\n",
        "  df = spark.read \\\n",
        "        .load(file_name, format=\"csv\", sep=\",\", header=header, schema=schema) \n",
        "    \n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbqP_fkQMr2R"
      },
      "source": [
        "def extract_data_from_url(url: str, header: bool=False, schema: StructType=route_schema):\n",
        "  \"\"\"\n",
        "  extracts data from a url, by downloading the file and extracting the data\n",
        "  @param url: the url where teh file is located\n",
        "  \"\"\"\n",
        "  log.info(\"extract data from url\")\n",
        "\n",
        "  file_name = os.path.split(url)[1]\n",
        "  urllib.request.urlretrieve(url, file_name)\n",
        "  \n",
        "  df = extract_data(file_name, header, schema)\n",
        "  \n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3bZ6rXKNJOD"
      },
      "source": [
        "def transform_data(df: DataFrame):\n",
        "  \"\"\"\n",
        "  writes the data from a dataframe to a file\n",
        "  transforms the data to get the top 10 airports that are used most often as source airport\n",
        "  @param df: the input data\n",
        "  \"\"\"\n",
        "  log.info(\"transform data\")\n",
        "\n",
        "  result = df.groupBy(col(\"Source airport\")) \\\n",
        "      .count() \\\n",
        "      .orderBy(col(\"count\").desc()) \\\n",
        "      .limit(10)\n",
        "  \n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV0NQ6V_NLxg"
      },
      "source": [
        "def load_data(df: DataFrame, file: str, file_format: str=\"csv\"):\n",
        "  \"\"\"\n",
        "  @param df: the data\n",
        "  @param file: the file name\n",
        "  @param file_format: the format of the output file\n",
        "  \"\"\"\n",
        "  log.info(\"load data to file\")\n",
        "\n",
        "  df.write.format(file_format).mode('overwrite') \\\n",
        "      .option(\"header\", \"true\") \\\n",
        "      .save(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi6WhqIHZldR"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imUBqrj_OZt2"
      },
      "source": [
        "##Results Task 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV2lsPCkNQi6"
      },
      "source": [
        "def task1():\n",
        "  \"\"\"\n",
        "  synchronously execute the steps for the taks 1 job\n",
        "  \"\"\"\n",
        "  log.info(\"start task 1\")\n",
        "\n",
        "  df = extract_data_from_url(url)\n",
        "  result = transform_data(df)\n",
        "  result.show()\n",
        "  load_data(result, \"output.csv\", \"csv\")\n",
        "\n",
        "  log.info(\"task 1 completed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSAOKwKXNZeY",
        "outputId": "ed16c6b3-2d4b-401c-9633-35fa3b24067f"
      },
      "source": [
        "task1()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:start task 1\n",
            "INFO:__main__:extract data from url\n",
            "INFO:__main__:extract data from file\n",
            "INFO:__main__:transform data\n",
            "INFO:__main__:load data to file\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+--------------+-----+\n",
            "|Source airport|count|\n",
            "+--------------+-----+\n",
            "|           ATL|  915|\n",
            "|           ORD|  558|\n",
            "|           PEK|  535|\n",
            "|           LHR|  527|\n",
            "|           CDG|  524|\n",
            "|           FRA|  497|\n",
            "|           LAX|  492|\n",
            "|           DFW|  469|\n",
            "|           JFK|  456|\n",
            "|           AMS|  453|\n",
            "+--------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:task 1 completed\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws5y23ZXP_Ug"
      },
      "source": [
        "# Task 2 \n",
        "*Use Spark structured streaming to change your job into a streaming job, and use the dataset file as a source.*\n",
        "\n",
        "I used the RatestreamSource to create a stream of the routes with a timestamp. By joining those two together on row nmber I get a stream of routes with an event time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPbNCCJyYM8b"
      },
      "source": [
        "def get_stream(rowsPerSecond: int=1000):\n",
        "  \"\"\"\n",
        "  Gets a dummy stream of x rows per second\n",
        "  @param rowsPerSecond: number of rows per second that should be outputed\n",
        "  \"\"\"\n",
        "  log.info(\"get stream\")\n",
        "\n",
        "  stream = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"rate\") \\\n",
        "    .option(\"rowsPerSecond\", rowsPerSecond) \\\n",
        "    .option(\"numPartitions\", \"1\") \\\n",
        "    .option(\"rampUpTime\", \"0\") \\\n",
        "    .load()\n",
        "\n",
        "  return stream"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBP97Cm8ZEJH"
      },
      "source": [
        "def combine_to_stream():\n",
        "  \"\"\"\n",
        "  combine the dummy stream with the routes source data to create a stream of routes\n",
        "  \"\"\"\n",
        "  log.info(\"combine stream and source data\")\n",
        "\n",
        "  routes = extract_data_from_url(url)\n",
        "  routes = routes \\\n",
        "            .withColumn('rand', lit(\"\")) \\\n",
        "            .withColumn(\"row_num\", row_number().over(Window.partitionBy().orderBy(\"rand\")))\n",
        "\n",
        "  stream = get_stream()\n",
        "\n",
        "  return stream.join(routes, [(stream.value == routes.row_num)], how=\"inner\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6FbmJboQb2l"
      },
      "source": [
        "## Results Task 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG4Qs6NobMia"
      },
      "source": [
        "def task2():\n",
        "  \"\"\"\n",
        "  Task 2 combine the data from the source file with time data to create a tream of data\n",
        "  \"\"\"\n",
        "  log.info(\"start task 2\")\n",
        "\n",
        "  combine_to_stream()\n",
        "\n",
        "  log.info(\"task 2 completed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqTslbxCcpbc",
        "outputId": "480bf344-0c8b-484d-c392-286049eb3c50"
      },
      "source": [
        "task2()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:start task 2\n",
            "INFO:__main__:combine stream and source data\n",
            "INFO:__main__:extract data from url\n",
            "INFO:__main__:extract data from file\n",
            "INFO:__main__:get stream\n",
            "INFO:__main__:task 2 completed\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKWsT-rnQlIS"
      },
      "source": [
        "# Task 3\n",
        "*Next change your streaming job so the aggregations are done using sliding windows. Pick any window and sliding interval. The end result should be the top 10 airports used as source airport within each window. When choosing the window interval, keep the size of the dataset in mind.*\n",
        "\n",
        "I used Pyspark Structure streaming, however this has some limitations with limit and order by, which can not easily be used in the streaming query. Therefore, I store the stream with the windows and bathc process the data in top 10's. With the function foreach_batch_function, each batch is stored after it is available, this function can easily be extended to store to multiple locations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRcWE43Fkkke"
      },
      "source": [
        "def transform_data_window(df: DataFrame):\n",
        "  \"\"\"\n",
        "  transforms the stremaing data to windowed data with counts for that window and source airport\n",
        "  @param df: the input data\n",
        "  \"\"\"\n",
        "  log.info(\"transform data to windowing data\")\n",
        "\n",
        "  result = df \\\n",
        "    .withColumn(\"timestamp\", current_timestamp()) \\\n",
        "    .withWatermark(\"timestamp\", \"5 seconds\") \\\n",
        "    .groupBy(window(col(\"timestamp\"), \"10 seconds\"), col(\"Source airport\")) \\\n",
        "    .count() \\\n",
        "    .select(\"window.start\", \"window.end\", \"Source airport\", \"count\")    \n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAxdem_Nczjq"
      },
      "source": [
        "def foreach_batch_function(batch_df: DataFrame, batch_id: int, output_file_name: str):\n",
        "  \"\"\"\n",
        "  process each batch and store the data to file, by overwriting the previous data\n",
        "  in 1 partition, to easily check for a human\n",
        "  @param batch_df: the data of the batch\n",
        "  @param bath_id: the batch id\n",
        "  @param output_file_name: teh name where the batch should be stored\n",
        "  \"\"\"\n",
        "  log.info(\"write batch to file\")\n",
        "\n",
        "  batch_df \\\n",
        "    .repartition(1) \\\n",
        "    .write.mode(\"overwrite\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .csv(output_file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w0aBOyb-G0P"
      },
      "source": [
        "def start_streaming_load(transformation: DataFrame, output_file_name: str, max_secs: int=100):\n",
        "  \"\"\"\n",
        "  starts the streaming for a max secs and loads the windowed data to an file\n",
        "  @param transformation: the transformation data\n",
        "  @param output_file_name: output file name where the data should be stored\n",
        "  @param max_secs: max seconds to run for this streaming demo\n",
        "  \"\"\"\n",
        "  log.info(f\"saving stream to file for max {max_secs} secs\")\n",
        "\n",
        "  query = transformation \\\n",
        "    .writeStream \\\n",
        "    .foreachBatch(lambda batch_df, batch_id: foreach_batch_function(batch_df, batch_id, output_file_name)) \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .start()\n",
        "\n",
        "  t = 0\n",
        "\n",
        "  while t <= max_secs:\n",
        "      sleep_secs = 5\n",
        "      time.sleep(sleep_secs)\n",
        "      t += sleep_secs\n",
        "\n",
        "  query.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isIncKo6-9_D"
      },
      "source": [
        "def transform_windowed_data(df: DataFrame):\n",
        "  \"\"\"\n",
        "  transforms the window data into top 10's per window by partitioning\n",
        "  @param df: the input data\n",
        "  \"\"\"\n",
        "  log.info(\"transform data to top 10 per window\")\n",
        "\n",
        "  result = df.withColumn(\"window_rank\", \\\n",
        "                          row_number().over(Window.partitionBy(\"start\").orderBy(col(\"count\").desc(), col(\"Source airport\")))) \\\n",
        "      .filter(col(\"window_rank\") <= 10) \\\n",
        "      .orderBy(col(\"start\"), col(\"count\").desc())\n",
        "  \n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_0BG19CQ538"
      },
      "source": [
        "## Results Task 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTI-i1LUNcHP"
      },
      "source": [
        "def task3():\n",
        "  \"\"\"\n",
        "  synchronously execute the steps for the taks 3 job by loading the stream to file\n",
        "  and partition the data from the windows into top 10's, since the current PySpark\n",
        "  Structure streaming does not allow order by and limits in the streaming query\n",
        "  \"\"\"\n",
        "  log.info(\"start task 3\")\n",
        "\n",
        "  output_file_name = \"stream_batch_all.csv\"\n",
        "\n",
        "  stream = combine_to_stream()\n",
        "  transform = transform_data_window(stream)\n",
        "  start_streaming_load(transform, output_file_name)\n",
        "  \n",
        "  result_schema = StructType([ \\\n",
        "    StructField(\"start\", TimestampType(), True), \\\n",
        "    StructField(\"end\", TimestampType(), True), \\\n",
        "    StructField(\"Source airport\", StringType(),True), \\\n",
        "    StructField(\"count\", IntegerType(), True)\n",
        "  ])\n",
        "\n",
        "  df = extract_data(output_file_name, header=True, schema=result_schema)\n",
        "  df_top10 = transform_windowed_data(df)\n",
        "  data_table.DataTable(df_top10.toPandas(), include_index=False, num_rows_per_page=10)\n",
        "\n",
        "  log.info(\"task 3 completed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExSSU_Bul4pV",
        "outputId": "c878e0c3-ca9a-4391-ed67-d8bbcf5d60ff"
      },
      "source": [
        "task3()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:start task 3\n",
            "INFO:__main__:combine stream and source data\n",
            "INFO:__main__:extract data from url\n",
            "INFO:__main__:extract data from file\n",
            "INFO:__main__:get stream\n",
            "INFO:__main__:transform data to windowing data\n",
            "INFO:__main__:saving stream to file for max 100 secs\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:extract data from file\n",
            "INFO:__main__:transform data to top 10 per window\n",
            "INFO:__main__:task 3 completed\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "ESvqkwfSWen-",
        "outputId": "54c1a07f-407c-4ff7-c8f4-1898ac728e70"
      },
      "source": [
        "output_file_name = \"stream_batch_all.csv\"\n",
        "\n",
        "stream = combine_to_stream()\n",
        "transform = transform_data_window(stream)\n",
        "start_streaming_load(transform, output_file_name)\n",
        "\n",
        "result_schema = StructType([ \\\n",
        "  StructField(\"start\", TimestampType(), True), \\\n",
        "  StructField(\"end\", TimestampType(), True), \\\n",
        "  StructField(\"Source airport\", StringType(),True), \\\n",
        "  StructField(\"count\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "df = extract_data(output_file_name, header=True, schema=result_schema)\n",
        "df_top10 = transform_windowed_data(df)\n",
        "data_table.DataTable(df_top10.toPandas(), include_index=False, num_rows_per_page=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:combine stream and source data\n",
            "INFO:__main__:extract data from url\n",
            "INFO:__main__:extract data from file\n",
            "INFO:__main__:get stream\n",
            "INFO:__main__:transform data to windowing data\n",
            "INFO:__main__:saving stream to file for max 100 secs\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:write batch to file\n",
            "INFO:__main__:extract data from file\n",
            "INFO:__main__:transform data to top 10 per window\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[\"2021-05-05 14:37:00\",\n\"2021-05-05 14:37:10\",\n\"DFW\",\n{\n            'v': 184,\n            'f': \"184\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }],\n [\"2021-05-05 14:37:00\",\n\"2021-05-05 14:37:10\",\n\"CLT\",\n{\n            'v': 135,\n            'f': \"135\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        }],\n [\"2021-05-05 14:37:00\",\n\"2021-05-05 14:37:10\",\n\"MIA\",\n{\n            'v': 127,\n            'f': \"127\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        }],\n [\"2021-05-05 14:37:00\",\n\"2021-05-05 14:37:10\",\n\"ORD\",\n{\n            'v': 126,\n            'f': \"126\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        }],\n [\"2021-05-05 14:37:00\",\n\"2021-05-05 14:37:10\",\n\"PHL\",\n{\n            'v': 119,\n            'f': \"119\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        }],\n [\"2021-05-05 14:37:00\",\n\"2021-05-05 14:37:10\",\n\"LHR\",\n{\n            'v': 101,\n            'f': \"101\",\n        },\n{\n            'v': 6,\n            'f': \"6\",\n        }],\n [\"2021-05-05 14:37:00\",\n\"2021-05-05 14:37:10\",\n\"DCA\",\n{\n            'v': 75,\n            'f': \"75\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        }],\n [\"2021-05-05 14:37:00\",\n\"2021-05-05 14:37:10\",\n\"BOM\",\n{\n            'v': 74,\n            'f': \"74\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        }],\n [\"2021-05-05 14:37:00\",\n\"2021-05-05 14:37:10\",\n\"PHX\",\n{\n            'v': 74,\n            'f': \"74\",\n        },\n{\n            'v': 9,\n            'f': \"9\",\n        }],\n [\"2021-05-05 14:37:00\",\n\"2021-05-05 14:37:10\",\n\"ATH\",\n{\n            'v': 71,\n            'f': \"71\",\n        },\n{\n            'v': 10,\n            'f': \"10\",\n        }],\n [\"2021-05-05 14:37:10\",\n\"2021-05-05 14:37:20\",\n\"ATL\",\n{\n            'v': 313,\n            'f': \"313\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }],\n [\"2021-05-05 14:37:10\",\n\"2021-05-05 14:37:20\",\n\"CDG\",\n{\n            'v': 258,\n            'f': \"258\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        }],\n [\"2021-05-05 14:37:10\",\n\"2021-05-05 14:37:20\",\n\"LHR\",\n{\n            'v': 191,\n            'f': \"191\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        }],\n [\"2021-05-05 14:37:10\",\n\"2021-05-05 14:37:20\",\n\"YYZ\",\n{\n            'v': 131,\n            'f': \"131\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        }],\n [\"2021-05-05 14:37:10\",\n\"2021-05-05 14:37:20\",\n\"JFK\",\n{\n            'v': 126,\n            'f': \"126\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        }],\n [\"2021-05-05 14:37:10\",\n\"2021-05-05 14:37:20\",\n\"FCO\",\n{\n            'v': 125,\n            'f': \"125\",\n        },\n{\n            'v': 6,\n            'f': \"6\",\n        }],\n [\"2021-05-05 14:37:10\",\n\"2021-05-05 14:37:20\",\n\"MEX\",\n{\n            'v': 96,\n            'f': \"96\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        }],\n [\"2021-05-05 14:37:10\",\n\"2021-05-05 14:37:20\",\n\"DUS\",\n{\n            'v': 88,\n            'f': \"88\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        }],\n [\"2021-05-05 14:37:10\",\n\"2021-05-05 14:37:20\",\n\"HEL\",\n{\n            'v': 88,\n            'f': \"88\",\n        },\n{\n            'v': 9,\n            'f': \"9\",\n        }],\n [\"2021-05-05 14:37:10\",\n\"2021-05-05 14:37:20\",\n\"TXL\",\n{\n            'v': 88,\n            'f': \"88\",\n        },\n{\n            'v': 10,\n            'f': \"10\",\n        }],\n [\"2021-05-05 14:37:20\",\n\"2021-05-05 14:37:30\",\n\"ATL\",\n{\n            'v': 215,\n            'f': \"215\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }],\n [\"2021-05-05 14:37:20\",\n\"2021-05-05 14:37:30\",\n\"PEK\",\n{\n            'v': 210,\n            'f': \"210\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        }],\n [\"2021-05-05 14:37:20\",\n\"2021-05-05 14:37:30\",\n\"CAN\",\n{\n            'v': 168,\n            'f': \"168\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        }],\n [\"2021-05-05 14:37:20\",\n\"2021-05-05 14:37:30\",\n\"DTW\",\n{\n            'v': 134,\n            'f': \"134\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        }],\n [\"2021-05-05 14:37:20\",\n\"2021-05-05 14:37:30\",\n\"TPE\",\n{\n            'v': 129,\n            'f': \"129\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        }],\n [\"2021-05-05 14:37:20\",\n\"2021-05-05 14:37:30\",\n\"MSP\",\n{\n            'v': 127,\n            'f': \"127\",\n        },\n{\n            'v': 6,\n            'f': \"6\",\n        }],\n [\"2021-05-05 14:37:20\",\n\"2021-05-05 14:37:30\",\n\"HKG\",\n{\n            'v': 111,\n            'f': \"111\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        }],\n [\"2021-05-05 14:37:20\",\n\"2021-05-05 14:37:30\",\n\"SZX\",\n{\n            'v': 99,\n            'f': \"99\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        }],\n [\"2021-05-05 14:37:20\",\n\"2021-05-05 14:37:30\",\n\"JFK\",\n{\n            'v': 98,\n            'f': \"98\",\n        },\n{\n            'v': 9,\n            'f': \"9\",\n        }],\n [\"2021-05-05 14:37:20\",\n\"2021-05-05 14:37:30\",\n\"CTU\",\n{\n            'v': 95,\n            'f': \"95\",\n        },\n{\n            'v': 10,\n            'f': \"10\",\n        }],\n [\"2021-05-05 14:37:30\",\n\"2021-05-05 14:37:40\",\n\"FRA\",\n{\n            'v': 245,\n            'f': \"245\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }],\n [\"2021-05-05 14:37:30\",\n\"2021-05-05 14:37:40\",\n\"AMS\",\n{\n            'v': 235,\n            'f': \"235\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        }],\n [\"2021-05-05 14:37:30\",\n\"2021-05-05 14:37:40\",\n\"ATL\",\n{\n            'v': 217,\n            'f': \"217\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        }],\n [\"2021-05-05 14:37:30\",\n\"2021-05-05 14:37:40\",\n\"DXB\",\n{\n            'v': 216,\n            'f': \"216\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        }],\n [\"2021-05-05 14:37:30\",\n\"2021-05-05 14:37:40\",\n\"BCN\",\n{\n            'v': 179,\n            'f': \"179\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        }],\n [\"2021-05-05 14:37:30\",\n\"2021-05-05 14:37:40\",\n\"MUC\",\n{\n            'v': 174,\n            'f': \"174\",\n        },\n{\n            'v': 6,\n            'f': \"6\",\n        }],\n [\"2021-05-05 14:37:30\",\n\"2021-05-05 14:37:40\",\n\"DUB\",\n{\n            'v': 166,\n            'f': \"166\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        }],\n [\"2021-05-05 14:37:30\",\n\"2021-05-05 14:37:40\",\n\"MAD\",\n{\n            'v': 160,\n            'f': \"160\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        }],\n [\"2021-05-05 14:37:30\",\n\"2021-05-05 14:37:40\",\n\"ZRH\",\n{\n            'v': 133,\n            'f': \"133\",\n        },\n{\n            'v': 9,\n            'f': \"9\",\n        }],\n [\"2021-05-05 14:37:30\",\n\"2021-05-05 14:37:40\",\n\"ICN\",\n{\n            'v': 129,\n            'f': \"129\",\n        },\n{\n            'v': 10,\n            'f': \"10\",\n        }],\n [\"2021-05-05 14:37:40\",\n\"2021-05-05 14:37:50\",\n\"PVG\",\n{\n            'v': 128,\n            'f': \"128\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }],\n [\"2021-05-05 14:37:40\",\n\"2021-05-05 14:37:50\",\n\"VIE\",\n{\n            'v': 118,\n            'f': \"118\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        }],\n [\"2021-05-05 14:37:40\",\n\"2021-05-05 14:37:50\",\n\"ICN\",\n{\n            'v': 112,\n            'f': \"112\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        }],\n [\"2021-05-05 14:37:40\",\n\"2021-05-05 14:37:50\",\n\"KUL\",\n{\n            'v': 98,\n            'f': \"98\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        }],\n [\"2021-05-05 14:37:40\",\n\"2021-05-05 14:37:50\",\n\"KMG\",\n{\n            'v': 81,\n            'f': \"81\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        }],\n [\"2021-05-05 14:37:40\",\n\"2021-05-05 14:37:50\",\n\"CAI\",\n{\n            'v': 79,\n            'f': \"79\",\n        },\n{\n            'v': 6,\n            'f': \"6\",\n        }],\n [\"2021-05-05 14:37:40\",\n\"2021-05-05 14:37:50\",\n\"SIN\",\n{\n            'v': 79,\n            'f': \"79\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        }],\n [\"2021-05-05 14:37:40\",\n\"2021-05-05 14:37:50\",\n\"NRT\",\n{\n            'v': 76,\n            'f': \"76\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        }],\n [\"2021-05-05 14:37:40\",\n\"2021-05-05 14:37:50\",\n\"SAW\",\n{\n            'v': 75,\n            'f': \"75\",\n        },\n{\n            'v': 9,\n            'f': \"9\",\n        }],\n [\"2021-05-05 14:37:40\",\n\"2021-05-05 14:37:50\",\n\"PEK\",\n{\n            'v': 70,\n            'f': \"70\",\n        },\n{\n            'v': 10,\n            'f': \"10\",\n        }],\n [\"2021-05-05 14:37:50\",\n\"2021-05-05 14:38:00\",\n\"ORD\",\n{\n            'v': 290,\n            'f': \"290\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }],\n [\"2021-05-05 14:37:50\",\n\"2021-05-05 14:38:00\",\n\"IST\",\n{\n            'v': 260,\n            'f': \"260\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        }],\n [\"2021-05-05 14:37:50\",\n\"2021-05-05 14:38:00\",\n\"DME\",\n{\n            'v': 200,\n            'f': \"200\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        }],\n [\"2021-05-05 14:37:50\",\n\"2021-05-05 14:38:00\",\n\"DFW\",\n{\n            'v': 196,\n            'f': \"196\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        }],\n [\"2021-05-05 14:37:50\",\n\"2021-05-05 14:38:00\",\n\"LGW\",\n{\n            'v': 191,\n            'f': \"191\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        }],\n [\"2021-05-05 14:37:50\",\n\"2021-05-05 14:38:00\",\n\"IAH\",\n{\n            'v': 177,\n            'f': \"177\",\n        },\n{\n            'v': 6,\n            'f': \"6\",\n        }],\n [\"2021-05-05 14:37:50\",\n\"2021-05-05 14:38:00\",\n\"SIN\",\n{\n            'v': 169,\n            'f': \"169\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        }],\n [\"2021-05-05 14:37:50\",\n\"2021-05-05 14:38:00\",\n\"EWR\",\n{\n            'v': 156,\n            'f': \"156\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        }],\n [\"2021-05-05 14:37:50\",\n\"2021-05-05 14:38:00\",\n\"LAX\",\n{\n            'v': 146,\n            'f': \"146\",\n        },\n{\n            'v': 9,\n            'f': \"9\",\n        }],\n [\"2021-05-05 14:37:50\",\n\"2021-05-05 14:38:00\",\n\"LIS\",\n{\n            'v': 146,\n            'f': \"146\",\n        },\n{\n            'v': 10,\n            'f': \"10\",\n        }],\n [\"2021-05-05 14:38:00\",\n\"2021-05-05 14:38:10\",\n\"PHL\",\n{\n            'v': 130,\n            'f': \"130\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }],\n [\"2021-05-05 14:38:00\",\n\"2021-05-05 14:38:10\",\n\"PHX\",\n{\n            'v': 126,\n            'f': \"126\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        }],\n [\"2021-05-05 14:38:00\",\n\"2021-05-05 14:38:10\",\n\"BCN\",\n{\n            'v': 117,\n            'f': \"117\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        }],\n [\"2021-05-05 14:38:00\",\n\"2021-05-05 14:38:10\",\n\"ATL\",\n{\n            'v': 110,\n            'f': \"110\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        }],\n [\"2021-05-05 14:38:00\",\n\"2021-05-05 14:38:10\",\n\"LAS\",\n{\n            'v': 74,\n            'f': \"74\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        }],\n [\"2021-05-05 14:38:00\",\n\"2021-05-05 14:38:10\",\n\"MDW\",\n{\n            'v': 66,\n            'f': \"66\",\n        },\n{\n            'v': 6,\n            'f': \"6\",\n        }],\n [\"2021-05-05 14:38:00\",\n\"2021-05-05 14:38:10\",\n\"YYZ\",\n{\n            'v': 64,\n            'f': \"64\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        }],\n [\"2021-05-05 14:38:00\",\n\"2021-05-05 14:38:10\",\n\"CTU\",\n{\n            'v': 60,\n            'f': \"60\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        }],\n [\"2021-05-05 14:38:00\",\n\"2021-05-05 14:38:10\",\n\"DEN\",\n{\n            'v': 56,\n            'f': \"56\",\n        },\n{\n            'v': 9,\n            'f': \"9\",\n        }],\n [\"2021-05-05 14:38:00\",\n\"2021-05-05 14:38:10\",\n\"SGN\",\n{\n            'v': 55,\n            'f': \"55\",\n        },\n{\n            'v': 10,\n            'f': \"10\",\n        }],\n [\"2021-05-05 14:38:10\",\n\"2021-05-05 14:38:20\",\n\"PEK\",\n{\n            'v': 83,\n            'f': \"83\",\n        },\n{\n            'v': 1,\n            'f': \"1\",\n        }],\n [\"2021-05-05 14:38:10\",\n\"2021-05-05 14:38:20\",\n\"SZX\",\n{\n            'v': 51,\n            'f': \"51\",\n        },\n{\n            'v': 2,\n            'f': \"2\",\n        }],\n [\"2021-05-05 14:38:10\",\n\"2021-05-05 14:38:20\",\n\"XIY\",\n{\n            'v': 27,\n            'f': \"27\",\n        },\n{\n            'v': 3,\n            'f': \"3\",\n        }],\n [\"2021-05-05 14:38:10\",\n\"2021-05-05 14:38:20\",\n\"NKG\",\n{\n            'v': 18,\n            'f': \"18\",\n        },\n{\n            'v': 4,\n            'f': \"4\",\n        }],\n [\"2021-05-05 14:38:10\",\n\"2021-05-05 14:38:20\",\n\"SHE\",\n{\n            'v': 17,\n            'f': \"17\",\n        },\n{\n            'v': 5,\n            'f': \"5\",\n        }],\n [\"2021-05-05 14:38:10\",\n\"2021-05-05 14:38:20\",\n\"SYD\",\n{\n            'v': 17,\n            'f': \"17\",\n        },\n{\n            'v': 6,\n            'f': \"6\",\n        }],\n [\"2021-05-05 14:38:10\",\n\"2021-05-05 14:38:20\",\n\"NNG\",\n{\n            'v': 16,\n            'f': \"16\",\n        },\n{\n            'v': 7,\n            'f': \"7\",\n        }],\n [\"2021-05-05 14:38:10\",\n\"2021-05-05 14:38:20\",\n\"PVG\",\n{\n            'v': 16,\n            'f': \"16\",\n        },\n{\n            'v': 8,\n            'f': \"8\",\n        }],\n [\"2021-05-05 14:38:10\",\n\"2021-05-05 14:38:20\",\n\"WUX\",\n{\n            'v': 16,\n            'f': \"16\",\n        },\n{\n            'v': 9,\n            'f': \"9\",\n        }],\n [\"2021-05-05 14:38:10\",\n\"2021-05-05 14:38:20\",\n\"DEN\",\n{\n            'v': 15,\n            'f': \"15\",\n        },\n{\n            'v': 10,\n            'f': \"10\",\n        }]],\n        columns: [[\"string\", \"start\"], [\"string\", \"end\"], [\"string\", \"Source airport\"], [\"number\", \"count\"], [\"number\", \"window_rank\"]],\n        columnOptions: [],\n        rowsPerPage: 10,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/plain": [
              "<google.colab.data_table.DataTable object>"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>Source airport</th>\n",
              "      <th>count</th>\n",
              "      <th>window_rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2021-05-05 14:37:00</td>\n",
              "      <td>2021-05-05 14:37:10</td>\n",
              "      <td>DFW</td>\n",
              "      <td>184</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2021-05-05 14:37:00</td>\n",
              "      <td>2021-05-05 14:37:10</td>\n",
              "      <td>CLT</td>\n",
              "      <td>135</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2021-05-05 14:37:00</td>\n",
              "      <td>2021-05-05 14:37:10</td>\n",
              "      <td>MIA</td>\n",
              "      <td>127</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2021-05-05 14:37:00</td>\n",
              "      <td>2021-05-05 14:37:10</td>\n",
              "      <td>ORD</td>\n",
              "      <td>126</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2021-05-05 14:37:00</td>\n",
              "      <td>2021-05-05 14:37:10</td>\n",
              "      <td>PHL</td>\n",
              "      <td>119</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>2021-05-05 14:38:10</td>\n",
              "      <td>2021-05-05 14:38:20</td>\n",
              "      <td>SYD</td>\n",
              "      <td>17</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>2021-05-05 14:38:10</td>\n",
              "      <td>2021-05-05 14:38:20</td>\n",
              "      <td>NNG</td>\n",
              "      <td>16</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>2021-05-05 14:38:10</td>\n",
              "      <td>2021-05-05 14:38:20</td>\n",
              "      <td>PVG</td>\n",
              "      <td>16</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>2021-05-05 14:38:10</td>\n",
              "      <td>2021-05-05 14:38:20</td>\n",
              "      <td>WUX</td>\n",
              "      <td>16</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>2021-05-05 14:38:10</td>\n",
              "      <td>2021-05-05 14:38:20</td>\n",
              "      <td>DEN</td>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80 rows × 5 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHmgCdu0Qoa_"
      },
      "source": [
        "# Task 4\n",
        "\n",
        "*Productionize your code by adding unit tests.*\n",
        "\n",
        "I added some simple unit test to cover the code. In the future this could be expanded to increase the code coverage. Also these relativly simple unit test could be expanded with more advanced assertions based on the dataframe equality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUB3Xp1iKP0P"
      },
      "source": [
        "## Results Task 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUaaZTK8XFwg"
      },
      "source": [
        "def test_transform_data():\n",
        "  \"\"\"\n",
        "  test function to test the tranform_data function if 10 rows are returned when > 10 rows are given\n",
        "  \"\"\"\n",
        "  data = [(\"GRQ\", \"\"),\n",
        "    (\"AMS\", \"\"),\n",
        "    (\"EIN\", \"\"),\n",
        "    (\"BCN\", \"\"),\n",
        "    (\"MAD\", \"\"),\n",
        "    (\"ORY\", \"\"),\n",
        "    (\"TEN\", \"\"),\n",
        "    (\"TNF\", \"\"),\n",
        "    (\"LON\", \"\"),\n",
        "    (\"FAR\", \"\"),\n",
        "    (\"DUB\", \"\"),\n",
        "    (\"TXL\", \"\"),\n",
        "    (\"AMS\", \"\")]\n",
        "\n",
        "  test_schema = StructType([ \\\n",
        "    StructField(\"Source airport\",StringType(), True), \\\n",
        "    StructField(\"Source\", StringType(), True)])\n",
        "\n",
        "  df = spark.createDataFrame(data=data, schema=test_schema)\n",
        "  actual_df = transform_data(df)\n",
        "  print(actual_df.count())\n",
        "\n",
        "  assert actual_df.count() == 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIZGbJ2h9wJX"
      },
      "source": [
        "def test_extract_data_from_url():\n",
        "  test_file_url = \"https://drive.google.com/uc?export=download&id=10Yfik6C8Eiyw9-flMbXAOiXwgLQPfc3w\"\n",
        "\n",
        "  test_schema = StructType([ \\\n",
        "    StructField(\"header1\",StringType(), True), \\\n",
        "    StructField(\"header2\", StringType(), True)])\n",
        "\n",
        "  data = [(\"Schiphol\", \"Data\"),\n",
        "      (\"Eindhoven Airport\", \"Logistiek\")]\n",
        "\n",
        "  df_expected = spark.createDataFrame(data=data, schema=test_schema)\n",
        "\n",
        "  df_actual = extract_data_from_url(test_file_url, header=True, schema=test_schema)\n",
        "\n",
        "  assert df_actual.count() == 2\n",
        "  assert_pyspark_df_equal(df_actual, df_expected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciw3R_ZC8Hta",
        "outputId": "99c13ba6-c626-49d4-b923-fd19b9c976e8"
      },
      "source": [
        "test_transform_data()\n",
        "test_extract_data_from_url()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:transform data\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:extract data from url\n",
            "INFO:__main__:extract data from file\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZOfN50cBWNk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}